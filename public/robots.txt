# ================================================
# Robots.txt für velwebsolutions.de
# VelWebSolutions - High-End Web Engineering
# Letzte Aktualisierung: 2026-01-03
# ================================================

User-agent: *
Allow: /
Allow: /_nuxt/
Allow: /static/

# Sitemaps
Sitemap: https://velwebsolutions.de/sitemap.xml

# ================================================
# BACKEND & SECURITY (Framework Protection)
# ================================================

# Laravel / PHP Internals
Disallow: /vendor/
Disallow: /storage/
Disallow: /bootstrap/
Disallow: /config/
Disallow: /database/
Disallow: /resources/
Disallow: /.env
Disallow: /artisan
Disallow: /composer.json
Disallow: /composer.lock
Disallow: /package.json
Disallow: /package-lock.json

# API & Auth
Disallow: /api/
Disallow: /sanctum/
Disallow: /_sanctum/
Disallow: /login
Disallow: /register
Disallow: /password/

# Admin & Dev
Disallow: /admin/
Disallow: /dashboard/
Disallow: /node_modules/
Disallow: /.git/
Disallow: /.github/
Disallow: /tmp/
Disallow: /logs/

# ================================================
# AGGRESSIVE BOTS & KI (Data Protection)
# ================================================

# Verhindert KI-Training durch deinen Fach-Content
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: CCBot
User-agent: Google-Extended
Disallow: /

# Verhindert unnötiges Crawling durch SEO-Tools (Crawl-Budget sparen)
User-agent: AhrefsBot
User-agent: SemrushBot
User-agent: Rogerbot
User-agent: DotBot
User-agent: MJ12bot
User-agent: SEOkicks
User-agent: BLEXBot
Disallow: /

# Verhindert aggressive chinesische & russische Bots
User-agent: Baiduspider
User-agent: YandexBot
Disallow: /

# ================================================
# CRAWL-DELAY (Server Performance Protection)
# ================================================

# Zwingt langsamere Bots zu einer Pause (schont LCP-Werte)
User-agent: *
Crawl-delay: 1